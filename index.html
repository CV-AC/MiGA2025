<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>MiGA-IJCAI2025</title>

  <link rel="shortcut icon" href="assets/images/miga.png">
  <link rel="stylesheet" href="assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/css/fontawsom-all.min.css">
  <link rel="stylesheet" href="assets/css/style.css?v=2">  <!-- bust cache -->
  <style>
    /* ===== Zoom info card (forced styles) ===== */
    .zoom-box{
      background:#e8f1ff !important;   /* 深一点的淡蓝 */
      border:2px solid #0B5ED7 !important;
      border-radius:12px;
      padding:14px 16px;
      margin:12px 0 20px;
    }
    .zoom-box h4{
      margin:0 0 8px;
      text-align:center;
      font-weight:700;
      color:#0B5ED7;
    }
    .zoom-box p{ margin-bottom:6px; }
    .zoom-box .label{ font-weight:700; color:#0B5ED7; }
    .zoom-join-btn{
      display:inline-block;
      padding:8px 12px;
      border-radius:8px;
      background:#0B5ED7;
      color:#fff !important;
      text-decoration:none;
      font-weight:700;
      margin-top:6px;
    }
  </style>
</head>

<body>
  <div class="container-fluid overcover">
    <div class="container profile-box">
      <div class="top-cover">
        <div class="covwe-inn">
          <div class="row no-margin">
            <div class="col-md-4 img-c">
              <img src="assets/images/miga.png" alt="">
            </div>
            <div class="col-md-8 tit-det">
              <h2>The 3rd Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA)</h2>
              <p>To be held at IJCAI 2025, 28th August 2025, Guangzhou, China</p>
            </div>
          </div>
        </div>
      </div>

      <ul class="nav nav-tabs navbar-expand-lg navbar-dark nav-fill" id="myTab" role="tablist">
        <li class="nav-item"><a class="nav-link active" id="home-tab" data-toggle="tab" href="#home" role="tab" aria-controls="home" aria-selected="true">Home</a></li>
        <li class="nav-item"><a class="nav-link" id="profile-tab" data-toggle="tab" href="#profile" role="tab" aria-controls="profile" aria-selected="false">Workshop</a></li>
        <li class="nav-item"><a class="nav-link" id="resume-tab" data-toggle="tab" href="#resume" role="tab" aria-controls="resume" aria-selected="false">Challenge</a></li>
        <li class="nav-item"><a class="nav-link" id="gallery-tab" data-toggle="tab" href="#gallery" role="tab" aria-controls="gallery" aria-selected="false">Organisers</a></li>
        <li class="nav-item"><a class="nav-link" id="contact-tab" data-toggle="tab" href="#contact" role="tab" aria-controls="contact" aria-selected="false">Contact</a></li>
      </ul>

      <div class="tab-content" id="myTabContent">
        <!-- ===================== HOME ===================== -->
        <div class="tab-pane fade show active" id="home" role="tabpanel" aria-labelledby="home-tab">
          <div class="row no-margin home-det">
            <!-- LEFT COLUMN -->
            <div class="col-md-4 big-img">
              <h4 class="ltitle">Welcome to MiGA Workshop & Challenge 2025</h4>
              <p>We jointly hold the third workshop and challenges for Micro-gesture Analysis for Hidden Emotion Understanding (MiGA) at IJCAI 2025, 28th August 2025, Guangzhou, China.</p>
              <p>We warmly welcome your contribution and participation!</p>

              <!-- ===== Zoom block inside Welcome module ===== -->
              <div class="zoom-box">
                <h4>Join Online (Zoom)</h4>
                <p><span class="label">Topic:</span> MiGA2025's Zoom Meeting</p>
                <p><span class="label">Time:</span> Aug 29, 2025 08:45 AM (Beijing/Shanghai)</p>
                <p><span class="label">Join Zoom:</span>
                  <a href="https://oulu.zoom.us/j/62508454882" target="_blank" rel="noopener noreferrer">
                    https://oulu.zoom.us/j/62508454882
                  </a>
                </p>
                <p><span class="label">Meeting ID:</span> 625 0845 4882</p>
                <a class="zoom-join-btn" href="https://oulu.zoom.us/j/62508454882" target="_blank" rel="noopener noreferrer">Join Zoom</a>
              </div>
              <!-- ===== end Zoom block ===== -->

              <h4 class="ltitle">News</h4>
              <div class="refer-cov">
                <p><b>12 August 2025</b> : Workshop schedule released. See the <a href="#profile" data-toggle="tab">Workshop</a> tab for details.</p>
              </div>
              <!--
              <div class="refer-cov"><p><b>15 June 2025</b> : Camera-ready paper submission deadline.</p></div>
              <div class="refer-cov"><p><b>10 June 2025</b> : Notification of acceptance sent to authors.</p></div>
              -->
              <div class="refer-cov"><p><b>07 June 2025</b> : Paper submission deadline is Extended!!.</p></div>
              <div class="refer-cov"><p><b>25 May 2025</b> : Challenge is officially over! Final results will be announced soon.</p></div>
              <div class="refer-cov"><p><b>30 March 2025</b> : The MiGA workshop & challenge website is live! Call for Challenge is now open.</p></div>
            </div>

            <!-- RIGHT COLUMN -->
            <div class="col-md-8 home-dat" style="text-align: justify;">
              <h2 class="rit-titl">Overview</h2>
              <p>We hold the 3rd MiGA Workshop & Challenge to <b>explore using body gestures for hidden emotional state analysis</b>, to be held at IJCAI 2025, 16th-22th August 2025, Montreal, Canada & Guangzhou, China.</p>
              <p>As an important non-verbal communicative fashion, human body gestures are capable of conveying emotional information during social communication. In previous works, efforts have been made mainly on facial expressions, speech, or expressive body gestures to interpret classical expressive emotions. Differently, we focus on a specific group of body gestures, called micro-gestures (MGs), used in the psychology research field to interpret inner human feelings.</p>

              <div class="profess-cover row no-margin"><div class="col-md-6"></div></div>

              <div style="text-align: center;">
                <img src="assets/dataset/iMiGUE_short2.gif" alt="Micro-gesture" width="500" height="300" class="center">
              </div>

              <p style="padding-top: 10px;"><b>MGs are subtle and spontaneous body movements that are proven, together with micro-expressions, to be more reliable than normal facial expressions for conveying hidden emotional information.</b></p>

              <p>The aim of our MiGA workshop & challenge is to build a united, supportive research community for micro-gesture analysis and related emotion understanding problems. It will facilitate discussions between different research labs in academia and industry, identify the main attributes that can vary between gesture-based emotional understanding, and discuss the progress that has been made in this field so far, while identifying the next immediate open problems the community should address. We provide two different datasets and related benchmarks and with to inspire a new way of utilizing body gestures for human emotion understanding and bring a new direction to the emotion AI community. </p>

              <div class="profess-cover row no-margin"><div class="col-md-6"></div></div>

              <div>
                <!-- ===== Schedule & Logistics ===== -->
                <h4 id="schedule">Schedule & Logistics</h4>
                <div class="alert alert-light" role="alert" style="border: 1px solid #eee;">
                  <p style="margin:0;"><b>Dear IJCAI-25 MiGA Workshop Participants,</b></p>
                  <p>As we approach the IJCAI conference, we would like to share some important information about the schedules, equipment, and logistics for the MiGA Workshop 2025.</p>
                  <p><b>Schedule.</b> The official MiGA workshop schedule is now available below. Please check your presentation time to ensure you are well-prepared.</p>
                  <p><b>Presentation.</b> Due to the compact schedule (half day), each accepted paper will have an <b>8-minute oral presentation</b>, followed by a <b>2-minute Q&amp;A</b> session.</p>
                  <p><b>Equipment &amp; Help.</b> The following standard facilities will be provided for your presentation, and a student helper will be on standby to assist with AV systems:</p>
                  <ul>
                    <li>Projector &amp; projection screens</li>
                    <li>A (Windows) laptop</li>
                    <li>HDMI cable</li>
                  </ul>
                  <p>We strongly recommend that presenters bring their own adapters (HDMI, Type-C, etc.) to avoid any disruptions to the presentation.</p>
                  <p>We look forward to seeing you soon at the MiGA Workshop!</p>
                </div>

                <h5 class="mt-4">IJCAI 2025 Workshop Schedule</h5>
                <ul style="margin-left: 1rem;">
                  <li><b>Session 1: Introduction</b>
                    <ul>
                      <li>9:00–9:15: Opening Remarks</li>
                      <li>9:15–9:40: Invited Talk: Artificial Emotional Intelligence with Multi-model Large Language Models (Prof. Zheng Lian, CAS)</li>
                      <li>9:40–9:50: Award ceremony</li>
                    </ul>
                  </li>
                  <li class="mt-2"><b>Session 2: MG recognition and classification</b>
                    <ul>
                      <li>9:50–10:00: Paper 1 Presentation: MM-Gesture: Towards Precise Micro-Gesture Recognition through Multimodal Fusion</li>
                      <li>10:00–10:10: Paper 2 Presentation: Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention</li>
                      <li>10:10–10:20: Paper 3 Presentation: Online Micro-Gesture Recognition in Long Videos via Spatiotemporal Feature Encoding and Query-Based Temporal Detection</li>
                      <li>10:20–10:30: Paper 4 Presentation: CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset</li>
                    </ul>
                  </li>
                  <li class="mt-2">10:30–11:00: <i>First Coffee Break</i></li>
                  <li class="mt-2"><b>Session 3: Human behaviour-based emotion analysis</b>
                    <ul>
                      <li>11:00–11:10: Paper 5 Presentation: Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition</li>
                      <li>11:10–11:20: Paper 6 Presentation: Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks</li>
                      <li>11:20–11:30: Paper 7 Presentation: Towards Fine-Grained Emotion Understanding via Skeleton-Based Micro-Gesture Recognition</li>
                      <li>11:30–11:40: Paper 8 Presentation: GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation</li>
                      <li>11:40–11:50: Paper 9 Presentation: Unreal Engine-based Data Augmentation to Improve Real-world Human Activity Recognition with Wearable Devices</li>
                    </ul>
                  </li>
                  <li class="mt-2">12:00–12:20: Free-form discussion</li>
                  <li>12:10–12:30: Wrap-up/Closing Remarks</li>
                  <li>12:30–14:00: Lunch Break</li>
                </ul>
              </div>

              <h4 style="margin-top:16px !important;">Workshop Topics</h4>
              <ul>
                The workshop supplementing the challenge covers a wider scope, i.e., <b>any paper that is related to human behaviour analysis and emotion understanding</b> can be submitted as a workshop paper (not limited to challenge winners!). The topic includes but is not limited to:
                <div style="margin-left: 20px; text-align: justify;">
                  <li style="list-style: disc;">Human behaviour and gesture analysis.</li>
                  <li style="list-style: disc;">Vision-based methodologies for emotion understanding, e.g., classification, detection, online recognition, generation, and transferring.</li>
                  <li style="list-style: disc;">Solutions for special challenges involved with the in-the-wild human behaviour analysis, e.g., severely imbalanced sample distribution, high heterogeneous samples of interclass, noisy irrelevant motions, noisy backgrounds, etc.</li>
                  <li style="list-style: disc;">Different modalities developed for emotion understanding, e.g., body gestures, attentive gazes, and desensitized voices.</li>
                  <li style="list-style: disc;">New data collected for the purpose of emotion understanding.</li>
                  <li style="list-style: disc;">Psychological study and neuroscience research about various body behaviors and their links to emotions.</li>
                  <li style="list-style: disc;">Applications of human behaviour analysis, e.g., for medical assessment in hospitals (ADHD, depression), for health surveillance at home or in other environments, for emotion assessment in various scenarios like for education, job interview, etc.</li>
                </div>
              </ul>

              <br>
              <h4>References</h4>
              <ul>
                <li style="list-style: disc;">Chen H., Shi H., Liu X., Li X., and Zhao G. SMG: A Micro-gesture Dataset Towards Spontaneous Body Gestures for Emotional Stress State Analysis. International Journal of Computer Vision (IJCV 2023), 2023: 1-21.</li>
                <li style="list-style: disc;">Liu, X., Shi H., Chen H., Yu Z., Li X., and Zhao G. "iMiGUE: An identity-free video dataset for micro-gesture understanding and emotion analysis." In Proceedings of CVPR 2021, pp. 10631-10642. 2021.</li>
                <li style="list-style: disc;">Chen H., Liu X., Li X., Shi H., & Zhao G. Analyze spontaneous gestures for emotional stress state recognition: A micro-gesture dataset and analysis with deep learning. FG 2019: 1-8.</li>
              </ul>

              <br>
              <h4>Related works</h4>
              <ul>
                <li style="list-style: disc;">Huang H., Wang Y., Linghu K., and Xia Z. Multi-modal Micro-gesture Classification via Multi-scale Heterogeneous Ensemble Network. MiGA-IJCAI workshop 2024.</li>
                <li style="list-style: disc;">Chen G., Wang F., Li K., Wu Z., Fan H., Yang Y., Wang M and Guo D. Prototype Learning for Micro-gesture Classification. MiGA-IJCAI workshop 2024.</li>
                <li style="list-style: disc;">Guo D., Li K., Hu B., Zhang Y., Wang M. Benchmarking Micro-action Recognition: Dataset, Methods, and Applications. TCSVT 2024.</li>
                <li style="list-style: disc;">Wang, Y., Dong Z., Li P., Liu Y. A Multimodal Micro-gesture Classification Model Based on CLIP. MiGA-IJCAI workshop 2024.</li>
              </ul>
            </div>
          </div>
        </div>

        <!-- ===================== WORKSHOP TAB ===================== -->
        <div class="tab-pane fade exp-cover" id="profile" role="tabpanel" aria-labelledby="profile-tab">
          <div class="data-box" style="text-align: justify;">
            <div class="sec-title">
              <h2 class="decorated"><span>Workshop Details</span></h2>
            </div>

            <h5>Introduction</h5>
            <p>Body gestures are an important form to reveal people’s emotions alongside facial expressions and speeches. For special occasions when people intend to control or hide their true feelings, e.g., for social etiquette or other reasons, body gestures are harder to control and thus the more revealing clues of actual feelings compared to the face and the voice. Microgestures (MG) are defined as a special category of body gestures that are indicative of humans’ emotional status. Representative instances include: scratching head, touching nose, and rubbing hands, which are not intended to be shown when communicating with others, but occur spontaneously due to e.g., felt stress or discomfort as shown in Figure 1. The MGs differ from indicative gestures which are performed on purpose for facilitating communications, e.g., using gestures to assist verbal expressions during a discussion. Although research on general body gestures is prevailing, it is largely about human’s macro body movement, lacking the finer level consideration such as the micro-gestures discussed above. As well the studies are mainly concerned with recognizing the movement performed expressively, and the link between gestures and hidden emotions is yet to be explored.</p>

            <div class="profess-cover row no-margin"><div class="col-md-6"></div></div>

            <div style="text-align: center;">
              <img src="assets/dataset/iMiGUE_short2.gif" alt="Micro-gesture" width="500" height="300" class="center">
            </div>
            <p>Fig. 1. Micro-gesture examples. A tennis player is talking while spontaneously performing the body gestures in the post-match interview.</p>

            <div class="profess-cover row no-margin"><div class="col-md-6"></div></div>

            <p>Motivated by the above observations, we propose to jointly hold the third MiGA challenge and workshop on micro-gesture analysis for hidden emotion understanding (MiGA) to fill the gap in the current research field. The MiGA workshop and challenge aim to promote research on developing AI methods for MG analysis toward the goal of hidden emotion understanding. As the proposed MiGA will be the second event, we propose to hold it in workshop + challenge mode with a focus on the competition part that builds and provides benchmark datasets and a fair validation platform for researchers working in the MG classification and online recognition for identity-insensitive emotion understanding. The workshop covers a wider scope than the challenge, i.e., any research that provides theoretical and practical support for gesture and micro-gesture analysis and emotion understanding.</p>

            <div class="profess-cover row no-margin"><div class="col-md-6"></div></div>

            <!-- ===== Schedule & Logistics duplicated in the Workshop tab before Topics ===== -->
            <h4 id="schedule-workshop">Schedule & Logistics</h4>
            <div class="alert alert-light" role="alert" style="border: 1px solid #eee;">
              <p><b>Dear IJCAI-25 MiGA Workshop Participants,</b></p>
              <p>As we approach the IJCAI conference, we would like to share some important information about the schedules, equipment, and logistics for the MiGA Workshop 2025.</p>
              <p><b>Schedule.</b> The official MiGA workshop schedule is now available below and also on our website: <a href="#schedule-workshop">MiGA Workshop Schedule</a>. Please check your presentation time to ensure you are well-prepared.</p>
              <p><b>Presentation.</b> Due to the compact schedule (half day), each accepted paper will have an <b>8-minute oral presentation</b>, followed by a <b>2-minute Q&amp;A</b> session.</p>
              <p><b>Equipment &amp; Help.</b> The following standard facilities will be provided for your presentation, and a student helper will be on standby to assist with AV systems:</p>
              <ul>
                <li>Projector &amp; projection screens</li>
                <li>A (Windows) laptop</li>
                <li>HDMI cable</li>
              </ul>
              <p>We strongly recommend that presenters bring their own adapters (HDMI, Type-C, etc.) to avoid any disruptions to the presentation.</p>
              <p>We look forward to seeing you soon at the MiGA Workshop!</p>
              <p>Best regards,<br>Haoyu Chen<br><span style="opacity:.8;">On behalf of the MiGA 2025 Committee</span></p>
            </div>

            <h5 class="mt-4">IJCAI 2025 Tentative Workshop Schedule</h5>
            <ul style="margin-left: 1rem;">
              <li><b>Session 1: Introduction</b>
                <ul>
                  <li>9:00–9:15: Opening Remarks</li>
                  <li>9:15–9:40: Invited Talk: Artificial Emotional Intelligence with Multi-model Large Language Models (Prof. Zheng Lian, CAS)</li>
                  <li>9:40–9:50: Award ceremony</li>
                </ul>
              </li>

              <li class="mt-2"><b>Session 2: MG recognition and classification</b>
                <ul>
                  <li>9:50–10:00: Paper 1 Presentation: MM-Gesture: Towards Precise Micro-Gesture Recognition through Multimodal Fusion</li>
                  <li>10:00–10:10: Paper 2 Presentation: Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention</li>
                  <li>10:10–10:20: Paper 3 Presentation: Online Micro-Gesture Recognition in Long Videos via Spatiotemporal Feature Encoding and Query-Based Temporal Detection</li>
                  <li>10:20–10:30: Paper 4 Presentation: CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset</li>
                </ul>
              </li>

              <li class="mt-2">10:30–11:00: <i>First Coffee Break</i></li>

              <li class="mt-2"><b>Session 3: Human behaviour-based emotion analysis</b>
                <ul>
                  <li>11:00–11:10: Paper 5 Presentation: Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition</li>
                  <li>11:10–11:20: Paper 6 Presentation: Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks</li>
                  <li>11:20–11:30: Paper 7 Presentation: Towards Fine-Grained Emotion Understanding via Skeleton-Based Micro-Gesture Recognition</li>
                  <li>11:30–11:40: Paper 8 Presentation: GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation</li>
                  <li>11:40–11:50: Paper 9 Presentation: Unreal Engine-based Data Augmentation to Improve Real-world Human Activity Recognition with Wearable Devices</li>
                </ul>
              </li>

              <li class="mt-2">12:00–12:20: Free-form discussion</li>
              <li>12:10–12:30: Wrap-up/Closing Remarks</li>
              <li>12:30–14:00: Lunch Break</li>
            </ul>

            <h5 style="margin-top:16px !important;">Workshop Topics</h5>
            <ul>
              The workshop supplementing the challenge covers a wider scope, i.e., <b>any paper that is related to human behaviour analysis and emotion understanding</b> can be submitted as a workshop paper(not limited to challenge winners!). The topic includes but is not limited to:
              <div style="margin-left: 20px; text-align: justify;">
                <li style="list-style: disc;">Human behaviour and gesture analysis.</li>
                <li style="list-style: disc;">Vision-based methodologies for emotion understanding, e.g., classification, detection, online recognition, generation, and transferring.</li>
                <li style="list-style: disc;">Solutions for special challenges involved with the in-the-wild human behaviour analysis, e.g., severely imbalanced sample distribution, high heterogeneous samples of interclass, noisy irrelevant motions, noisy backgrounds, etc.</li>
                <li style="list-style: disc;">Different modalities developed for emotion understanding, e.g., body gestures, attentive gazes, and desensitized voices.</li>
                <li style="list-style: disc;">New data collected for the purpose of emotion understanding.</li>
                <li style="list-style: disc;">Psychological study and neuroscience research about various body behaviors and their links to emotions.</li>
                <li style="list-style: disc;">Applications of human behaviour analysis, e.g., for medical assessment in hospitals (ADHD, depression), for health surveillance at home or in other environments, for emotion assessment in various scenarios like for education, job interview, etc.</li>
              </div>
            </ul>

            <div class="profess-cover row no-margin"><div class="col-md-6"></div></div>

            <h5>Submission Guidelines</h5>
            <p>- Papers must comply with the <b><a style="color:blue;" href="https://ceur-ws.org/HOWTOSUBMIT.html" target="_blank" rel="noopener noreferrer"> CEURART paper style</a></b> (1 column) and can fall in one of the following categories:</p>
            <p>- Full research papers (minimum 7 pages)</p>
            <p>- Short research papers (4-6 pages)</p>
            <p>- Position papers (2 pages)</p>
            <p>- The CEURART template can be found on this <b><a style="color:blue;" href="https://it.overleaf.com/latex/templates/template-for-submissions-to-ceur-workshop-proceedings-ceur-ws-dot-org/wqyfdgftmcfw/" target="_blank" rel="noopener noreferrer"> Overleaf link</a></b>.</p>
            <p>- Accepted papers (after blind review of at least 2-3 experts) will be included in a volume of the CEUR Workshop Proceedings. We are also planning to organize a special issue, and the authors of the most interesting and relevant papers will be invited to submit an extended manuscript.</p>
            <p>- Workshop submissions will be handled by CMT submission system; the submission link is as follows: <b><a style="color:blue;" href="https://cmt3.research.microsoft.com/MiGAIJCAI2025" target="_blank" rel="noopener noreferrer"> Paper Submission</a></b>. All questions about submissions should be emailed to chen.haoyu at oulu.fi</p>

            <div class="profess-cover row no-margin"><div class="col-md-6"></div></div>

            <h5><b>Workshop Important Dates</b></h5>
            <ul>
              <li><b>June 07, 2025</b>. Paper submission deadline.</li>
              <li><b>June 10, 2025</b>. Notification to authors.</li>
              <li><b>June 15, 2025</b>. Camera-ready deadline.</li>
              <li><b>August 29, 2025</b>. Workshop day.</li>
            </ul>

            <div class="profess-cover row no-margin"><div class="col-md-6"></div></div>
            <h5>Workshop Program (TBD)</h5>
            <p>The proposed workshop (including the challenge) will be held as a half-day event, on August 29th, 2025, Guangzhou, China.</p>
          </div>
        </div>

        <!-- ===================== CHALLENGE TAB ===================== -->
        <div class="tab-pane fade exp-cover" id="resume" role="tabpanel" aria-labelledby="resume-tab">
          <div class="data-box" style="text-align: justify;">
            <div class="sec-title">
              <h2 class="decorated"><span>Challenge Details</span></h2>
            </div>

            <p>The MiGA Challenge is an ongoing annual event featuring multiple challenge tasks each year, with progressively larger-scale datasets. This year, in addition to the two fundamental micro-gesture analysis tasks—<b>classification</b> and <b>online recognition</b>, we are taking MiGA a step further by introducing a new task: <b>behavior-based emotion understanding</b>. The tracks of MiGA will be extended to leverage those identity-insensitive cues to achieve hidden emotion understanding in the future. The challenge will be based on two spontaneous datasets: One is the <b>SMG dataset</b>, introduced in IJCV2023’s paper <i>“SMG: A Micro-Gesture Dataset Towards Spontaneous Body Gestures for Emotional Stress State Analysis.”</i> and the other is the <b>iMiGUE dataset</b>, published in CVPR2021’s paper <i>“iMiGUE: An Identity-free Video Dataset for Micro-Gesture Understanding and Emotion Analysis.”</i>. For MiGA 2025, we have introduced three challenge tasks (Tracks), and participating teams are welcome to compete in one or more of them. The challenge will be organized on the Kaggle website.</p>

            <p><b>Track 1: Micro-gesture classification from short video clips</b>. The MG datasets were collected from in-the-wild settings. Compared to ordinary action/gesture data, MGs concern more fine-grained and subtle body movements that occur spontaneously in practical interactions. Thus, learning those fine-grained body movement patterns, handling imbalanced sample distribution of MGs, and distinguishing the high heterogeneous MG samples of interclass are the big challenges to be addressed.</p>

            <p><b>Track 2: Multimodality-based online micro-gesture recognition from long video sequences</b>. Unlike any existing online action/gesture recognition datasets in which samples are well aligned/performed in the sequence, MGs samples occur spontaneously in any combination or order, just like seen in daily communicative scenarios. Thus, the task of online micro-gesture recognition requires dealing with more complex body-movement transition patterns (e.g., co- occurrence of multiple MGs, incomplete MGs and complicated transitions between MGs, etc.) and detecting fine-grained MGs from irrelevant/context body movements, which poses new challenges that haven’t been considered in previous gesture research.</p>

            <p><b>Track 3: Multimodality behavior-based emotion recognition from long video sequences</b>. In this track, <b>participants will try to predict whether a tennis player won or lost the match based on their body behaviors from the video recordings of the public interview press</b>. Unlike any existing body behavior-based emotion recognition datasets in which emotions or behaviors are acted or performed intentionally, our task is about recognizing subjects’ hidden emotions based on body behaviors directly from in-the-wild interview press. Thus, the task of behavior-based emotion recognition requires dealing with more complex challenges, e.g., modeling reliable emotion-behavior probabilistic with suitable algorithms, mining fine-grained MGs from irrelevant/context body movements to achieve reliable emotion analysis, which hasn’t been considered in previous gesture research.</p>

            <h5>The rules and guidelines for the competition/challenge</h5>

            <b>1. Datasets</b><br>
            <p>Datasets for the proposed challenge are available. The MiGA challenge is planned as a continuous annual event. Two benchmark datasets published on IJCV 2023 and CVPR 2021 are available and will be used for the challenge. The first one is the SMG dataset published in IJCV2023 Spontaneous Micro-Gesture (SMG) dataset consists of 3,692 samples of 17 MGs. The MG clips are annotated from 40 long video sequences (10-15 minutes) with 821,056 frames in total. The datasets were collected from 40 subjects while narrating a fake and real story to elicit the emotional states. The participants are recorded collected by Kinect resulting in four modalities, RGB, 3D skeletal joints, depth and silhouette. In this workshop, we allow participants to use the skeleton, RGB or both modalities. <a style="color: blue;" href="https://github.com/mikecheninoulu/SMG" target="_blank" rel="noopener noreferrer"> Details about SMG dataset</a></p>

            <p>The second dataset is iMiGUE published in CVPR2021. Micro-Gesture Understanding and Emotion analysis (iMiGUE) dataset consists of 32 MGs plus one non-MG class collected from post-match press conferences videos of famous tennis players. The dataset consists of 18,499 samples of MGs to detect negative and positive emotions. The MG clips are annotated from 359 long video sequences (0.5-26 minutes) with 3,765,600 frames in total. The dataset contains RGB modality and 2D skeletal joints collected from Open-Pose. In this workshop, we allow participants to use the skeleton, RGB or both modalities. <a style="color: blue;" href="https://github.com/linuxsino/iMiGUE" target="_blank" rel="noopener noreferrer"> Details about iMiGUE dataset</a></p>

            <p>Note that: 1) not all data are used for the challenge. 2) part of the data will be selected and tailored for different challenge tasks, please follow the Kaggle competition links to obtain and process the datasets.</p>

            <b>2. Evaluation</b><br>
            <p>We deploy a cross-subject evaluation protocol. For MG classification track, 13,936 and 3,692 MG clips from iMiGUE and SMG datasets will be used for training and validating, and the remaining 4,563 MG clips from iMiGUE will be used for testing. For MG online recognition track, 252 and 40 long sequences from iMiGUE and SMG datasets will be used for training and validating, and the remaining 104 long sequences from iMiGUE will be used for testing.</p>

            <p><b>MG classification track:</b> We report Top-1 accuracy on the testing setes on the iMiGUE dataset. Submissions will be ranked based on Top-1 accuracy on the overall split (if Top-1 results are the same, then Top-5 will be used to compare the results).</p>

            <p><b>MG online recognition track:</b> We jointly evaluate the detection and classification performances of algorithms by using the F1 score measurement defined below: F1 = 2 * Precision * Recall / (Precision + Recall), given a long video sequence that needs to be evaluated, Precision is the fraction of correctly classified MGs among all gestures retrieved in the sequence by algorithms, while Recall (or sensitivity) is the fraction of MGs that have been correctly retrieved over the total amount of annotated MGs.</p>

            <p><b>Behavior-based emotion prediction track:</b> We evaluate the submitted algorithms with a binary classification (win/lose) accuracy. The algorithm needs to predict the correct emotion state (positive/negative) corresponding to the match results (win/lose) of the tennis players based on their body behaviors during the interview press. Submissions will be ranked on the basis of the accuracy on the testing set.</p>

            <p>Submission format for three tracks. Participants must submit their predictions in a single .csv file on the Kaggle platform, more detailed instructions for each track can be found on Kaggle. The results will be evaluated on the server and displayed on the ranking list in real-time. The organization team has the right to examine the participants source code to ensure the reproducibility of the algorithms. The final results and ranking will be confirmed and announced by organizers.</p>

            <h5>Participation guidelines</h5>
            <p>Please visit the Kaggle websites to join the competitions:</p>
            <p><a style="color: blue;" href="https://kaggle.com/competitions/the-3rd-mi-ga-ijcai-challenge-track-1" target="_blank" rel="noopener noreferrer">The 3rd MiGA-IJCAI Challenge Track 1: Micro-gesture Classification</a></p>
            <p><a style="color: blue;" href="https://kaggle.com/competitions/the-3rd-mi-ga-ijcai-challenge-track-2" target="_blank" rel="noopener noreferrer">The 3rd MiGA-IJCAI Challenge Track 2: Micro-gesture Online Recognition</a></p>
            <p><a style="color: blue;" href="https://kaggle.com/competitions/the-3rd-mi-ga-ijcai-challenge-track-3" target="_blank" rel="noopener noreferrer">The 3rd MiGA-IJCAI Challenge Track 3: Behavior-based Emotion Prediction</a></p>

            <h5><b>Final results will be announced</b></h5>
            <p>The rankings for the <b>MiGA 2025</b> challenge will be published after the competition concludes. Once all submissions have been thoroughly reviewed, the final standings will be determined and announced. Stay tuned for updates!</p>

            <h5><b>Important Dates (might slightly adjust later)</b></h5>
            <ul>
              <li>The timeline for the competition will be organized as follows:</li>
              <li><b>Mar. 30th, 2025</b>. Call for Challenge online. (or earlier, based on the notification date)</li>
              <li><b>Apr. 05, 2025</b>. Release of training data.</li>
              <li><b>May. 11, 2025</b>. Release of testing data.</li>
              <li><b>May. 25, 2025</b>. Final test submission deadline.</li>
              <li><b>May. 27, 2025</b>. Release of challenge results.</li>
              <li><b>Jun. 07, 2025</b>. Paper submission deadline.</li>
              <li><b>Jun. 10, 2025</b>. Notification to authors.</li>
              <li><b>Jun. 15, 2025</b>. Camera-ready deadline.</li>
            </ul>
          </div>
        </div>

        <!-- ===================== ORGANISERS TAB ===================== -->
        <div class="tab-pane fade gallcoo" id="gallery" role="tabpanel" aria-labelledby="gallery-tab">
          <div class="sec-title">
            <h2 class="decorated"><span>Organizing Committee</span></h2>
          </div>

          <div class="cards">
            <div class="row justify-content-center">

              <!-- Person -->
              <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://gyzhao-nm.github.io/Guoying/" target="_blank" rel="noopener noreferrer">
                <div class="card-body">
                  <div class="card-front">
                    <div class="text-center">
                      <img class="img-fluid" src="assets/organizers/guoying-1.JPG" alt="Guoying Zhao" />
                      <h4>Guoying Zhao</h4>
                      <p>University of Oulu, FI <br></p>
                    </div>
                  </div>
                </div>
              </a>

              <!-- Person -->
              <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="http://www.schuller.one/" target="_blank" rel="noopener noreferrer">
                <div class="card-body">
                  <div class="card-front">
                    <div class="text-center">
                      <img class="img-fluid" src="assets/organizers/bjorn.png" alt="Björn W. Schuller" />
                      <h4>Björn W. Schuller</h4>
                      <p>University of Augsburg, DE <br> Imperial College London, UK</p>
                    </div>
                  </div>
                </div>
              </a>

              <!-- Person -->
              <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://stanford.edu/~eadeli/" target="_blank" rel="noopener noreferrer">
                <div class="card-body">
                  <div class="card-front">
                    <div class="text-center">
                      <img class="img-fluid" src="assets/organizers/ehsan.JPG" alt="Ehsan Adeli" />
                      <h4>Ehsan Adeli</h4>
                      <p>Stanford University, USA <br></p>
                    </div>
                  </div>
                </div>
              </a>

              <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://github.com/mikecheninoulu/" target="_blank" rel="noopener noreferrer">
                <div class="card-body">
                  <div class="card-front">
                    <div class="text-center">
                      <img class="img-fluid" src="assets/organizers/chenhaoyu1.png" alt="Haoyu Chen" />
                      <h4>Haoyu Chen</h4>
                      <p>University of Oulu, FI <br></p>
                    </div>
                  </div>
                </div>
              </a>

              <!-- (Optional) Hidden committee member commented in original -->
              <!--
              <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="http://sourcedb.psych.cas.cn/en/epsychexpert/201206/t20120628_3606154.html" target="_blank" rel="noopener noreferrer">
                <div class="card-body">
                  <div class="card-front">
                    <div class="text-center">
                      <img class="img-fluid" src="assets/organizers/tingshao-1.JPG" alt="Tingshao Zhu" />
                      <h4>Tingshao Zhu</h4>
                      <p>Institute of Psychology, Chinese Academy of Sciences <br></p>
                    </div>
                  </div>
                </div>
              </a>
              -->

            </div>
          </div>

          <div class="sec-title">
            <h2 class="decorated"><span>Invited speakers</span></h2>
          </div>

          <div class="cards">
            <div class="row justify-content-center">

              <!-- Person -->
              <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://zeroqiaoba.github.io/Homepage/" target="_blank" rel="noopener noreferrer">
                <div class="card-body">
                  <div class="card-front">
                    <div class="text-center">
                      <img class="img-fluid" src="assets/organizers/lianzheng.jpg" alt="Zheng Lian" />
                      <h4>Zheng Lian</h4>
                      <p>Institute of Automation, Chinese Academy of Sciences.<br></p>
                    </div>
                  </div>
                </div>
              </a>

              <!-- Person -->
              <a class="card col-xs-12 col-sm-12 col-md-6 col-lg-3 col-xl-3" href="https://www.jeffcohn.net/" target="_blank" rel="noopener noreferrer">
                <div class="card-body">
                  <div class="card-front">
                    <div class="text-center">
                      <img class="img-fluid" src="assets/organizers/jeffrey.jpg" alt="Jeffrey Cohn" />
                      <h4>Jeffrey Cohn</h4>
                      <p>University of Pittsburgh &amp; Carnegie Mellon University, US.<br></p>
                    </div>
                  </div>
                </div>
              </a>

            </div>
          </div>

          <div class="sec-title">
            <h2 class="decorated"><span>Data Chairs</span></h2>
          </div>

          <div class="cards">
            <div class="row justify-content-center">
              <!-- Person -->
              <a class="card col-lg-3" href="https://www.oulu.fi/en/researchers/fang-kang/" target="_blank" rel="noopener noreferrer">
                <div class="card-body">
                  <div class="card-front">
                    <div class="text-center">
                      <img class="img-fluid" src="assets/organizers/fangkang.jpg" alt="Fang Kang" />
                      <h4>Fang Kang</h4>
                      <p>University of Oulu, FI <br></p>
                    </div>
                  </div>
                </div>
              </a>
              <!-- Person -->
              <a class="card col-lg-3" href="https://www.oulu.fi/en/researchers/yueyi-yang" target="_blank" rel="noopener noreferrer">
                <div class="card-body">
                  <div class="card-front">
                    <div class="text-center">
                      <img class="img-fluid" src="assets/organizers/yueyiyang.jpg" alt="Yueyi Yang" />
                      <h4>Yueyi Yang</h4>
                      <p>University of Oulu, FI <br></p>
                    </div>
                  </div>
                </div>
              </a>
            </div>
          </div>

          <div class="sec-title">
            <h2 class="decorated"><span>Technical Support</span></h2>
          </div>

          <div class="cards">
            <div class="row justify-content-center">
              <!-- Person -->
              <a class="card col-lg-3" href="https://scholar.google.com.vn/citations?user=j_jye0QAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">
                <div class="card-body">
                  <div class="card-front">
                    <div class="text-center">
                      <img class="img-fluid" src="assets/organizers/lunah.jpeg" alt="Luna Huynh" />
                      <h4>Luna Huynh</h4>
                      <p>University of Oulu, FI <br></p>
                    </div>
                  </div>
                </div>
              </a>
              <!-- Person -->
              <a class="card col-lg-3" href="https://scholar.google.com.vn/citations?hl=en&amp;user=3Q5PfTkAAAAJ" target="_blank" rel="noopener noreferrer">
                <div class="card-body">
                  <div class="card-front">
                    <div class="text-center">
                      <img class="img-fluid" src="assets/organizers/boshra.jpeg" alt="Boshra Najari" />
                      <h4>Boshra Najari</h4>
                      <p>University of Oulu, FI <br></p>
                    </div>
                  </div>
                </div>
              </a>
              <!-- Person -->
              <a class="card col-lg-3" href="https://scholar.google.com.vn/citations?hl=en&amp;user=EzpyqQ4AAAAJ" target="_blank" rel="noopener noreferrer">
                <div class="card-body">
                  <div class="card-front">
                    <div class="text-center">
                      <img class="img-fluid" src="assets/organizers/bella.jpeg" alt="Belle Dang" />
                      <h4>Belle Dang</h4>
                      <p>University of Oulu, FI <br></p>
                    </div>
                  </div>
                </div>
              </a>
            </div>
          </div>
        </div>

        <!-- ===================== CONTACT TAB ===================== -->
        <div class="tab-pane fade contact-tab" id="contact" role="tabpanel" aria-labelledby="contact-tab">
          <div class="sec-title">
            <h2 class="decorated"><span>Contact us</span></h2>
          </div>

          <div class="row no-margin">
            <div class="col-md-6 no-padding" style="text-align: center;">
              <img src="assets/qr/qr_discord_2025.png" alt="Discord QR" width="300" height="300">
              <p>Welcome to our Discord channel and discuss with peers. <br>
                <a style="color:blue;" href="https://discord.gg/5yDyH3Xj" target="_blank" rel="noopener noreferrer">https://discord.gg/5yDyH3Xj</a>
              </p>
            </div>

            <div class="col-md-6">
              <ul>
                The contact info is listed as follows:
                <li>• For questions regarding <b>workshop submissions and the competition</b>, please get in touch with chen.haoyu@oulu.fi</li>
                <li>• For questions about the <b>workshop local arrangements and information</b>, please get in touch with pcchair@2025.ijcai.org</li>
                <li>• For questions regarding <b>general issues of the workshop program</b>, please get in touch with chen.haoyu@oulu.fi</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <script src="assets/js/jquery-3.2.1.min.js"></script>
  <script src="assets/js/popper.min.js"></script>
  <script src="assets/js/bootstrap.min.js"></script>
  <script src="assets/js/script.js"></script>
</body>
</html>