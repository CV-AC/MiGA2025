<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>MiGA-IJCAI2025</title>

  <link rel="shortcut icon" href="assets/images/miga.png">
  <link rel="stylesheet" href="assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/css/fontawsom-all.min.css">
  <link rel="stylesheet" href="assets/css/style.css?v=2">  <!-- bust cache -->
  <style>
    /* ===== Zoom info card (forced styles) ===== */
    .zoom-box{
      background:#e8f1ff !important;   /* 深一点的淡蓝 */
      border:2px solid #0B5ED7 !important;
      border-radius:12px;
      padding:14px 16px;
      margin:12px 0 20px;
    }
    .zoom-box h4{
      margin:0 0 8px;
      text-align:center;
      font-weight:700;
      color:#0B5ED7;
    }
    .zoom-box p{ margin-bottom:6px; }
    .zoom-box .label{ font-weight:700; color:#0B5ED7; }
    .zoom-join-btn{
      display:inline-block;
      padding:8px 12px;
      border-radius:8px;
      background:#0B5ED7;
      color:#fff !important;
      text-decoration:none;
      font-weight:700;
      margin-top:6px;
    }
  </style>
</head>

<body>
  <div class="container-fluid overcover">
    <div class="container profile-box">
      <div class="top-cover">
        <div class="covwe-inn">
          <div class="row no-margin">
            <div class="col-md-4 img-c">
              <img src="assets/images/miga.png" alt="">
            </div>
            <div class="col-md-8 tit-det">
              <h2>The 3rd Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA)</h2>
              <p>To be held at IJCAI 2025, 28th August 2025, Guangzhou, China</p>
            </div>
          </div>
        </div>
      </div>

      <ul class="nav nav-tabs navbar-expand-lg navbar-dark nav-fill" id="myTab" role="tablist">
        <li class="nav-item"><a class="nav-link active" id="home-tab" data-toggle="tab" href="#home" role="tab">Home</a></li>
        <li class="nav-item"><a class="nav-link" id="profile-tab" data-toggle="tab" href="#profile" role="tab">Workshop</a></li>
        <li class="nav-item"><a class="nav-link" id="resume-tab" data-toggle="tab" href="#resume" role="tab">Challenge</a></li>
        <li class="nav-item"><a class="nav-link" id="gallery-tab" data-toggle="tab" href="#gallery" role="tab">Organisers</a></li>
        <li class="nav-item"><a class="nav-link" id="contact-tab" data-toggle="tab" href="#contact" role="tab">Contact</a></li>
      </ul>

      <div class="tab-content" id="myTabContent">
        <!-- ===================== HOME ===================== -->
        <div class="tab-pane fade show active" id="home" role="tabpanel" aria-labelledby="home-tab">
          <div class="row no-margin home-det">
            <!-- LEFT COLUMN -->
            <div class="col-md-4 big-img">
              <h4 class="ltitle">Welcome to MiGA Workshop & Challenge 2025</h4>
              <p>We jointly hold the third workshop and challenges for Micro-gesture Analysis for Hidden Emotion Understanding (MiGA) at IJCAI 2025, 28th August 2025, Guangzhou, China.</p>
              <p>We warmly welcome your contribution and participation!</p>

              <!-- ===== Zoom block inside Welcome module ===== -->
              <div class="zoom-box">
                <h4>Join Online (Zoom)</h4>
                <p><span class="label">Topic:</span> MiGA2025's Zoom Meeting</p>
                <p><span class="label">Time:</span> Aug 29, 2025 08:45 AM (Beijing/Shanghai)</p>
                <p><span class="label">Join Zoom:</span>
                  <a href="https://oulu.zoom.us/j/62508454882" target="_blank" rel="noopener noreferrer">
                    https://oulu.zoom.us/j/62508454882
                  </a>
                </p>
                <p><span class="label">Meeting ID:</span> 625 0845 4882</p>
                <a class="zoom-join-btn" href="https://oulu.zoom.us/j/62508454882" target="_blank" rel="noopener noreferrer">Join Zoom</a>
              </div>
              <!-- ===== end Zoom block ===== -->

              <h4 class="ltitle">News</h4>
              <div class="refer-cov">
                <p><b>12 August 2025</b> : Workshop schedule released. See the <a href="#profile" data-toggle="tab">Workshop</a> tab for details.</p>
              </div>
              <!--
              <div class="refer-cov"><p><b>15 June 2025</b> : Camera-ready paper submission deadline.</p></div>
              <div class="refer-cov"><p><b>10 June 2025</b> : Notification of acceptance sent to authors.</p></div>
              -->
              <div class="refer-cov"><p><b>07 June 2025</b> : Paper submission deadline is Extended!!.</p></div>
              <div class="refer-cov"><p><b>25 May 2025</b> : Challenge is officially over! Final results will be announced soon.</p></div>
              <div class="refer-cov"><p><b>30 March 2025</b> : The MiGA workshop & challenge website is live! Call for Challenge is now open.</p></div>
            </div>

            <!-- RIGHT COLUMN -->
            <div class="col-md-8 home-dat" style="text-align: justify;">
              <h2 class="rit-titl">Overview</h2>
              <p>We hold the 3rd MiGA Workshop & Challenge to <b>explore using body gestures for hidden emotional state analysis</b>, to be held at IJCAI 2025, 16th-22th August 2025, Montreal, Canada & Guangzhou, China.</p>
              <p>As an important non-verbal communicative fashion, human body gestures are capable of conveying emotional information during social communication. In previous works, efforts have been made mainly on facial expressions, speech, or expressive body gestures to interpret classical expressive emotions. Differently, we focus on a specific group of body gestures, called micro-gestures (MGs), used in the psychology research field to interpret inner human feelings.</p>

              <div class="profess-cover row no-margin"><div class="col-md-6"></div></div>

              <div style="text-align: center;">
                <img src="assets/dataset/iMiGUE_short2.gif" alt="Micro-gesture" width="500" height="300" class="center">
              </div>

              <p style="padding-top: 10px;"><b>MGs are subtle and spontaneous body movements that are proven, together with micro-expressions, to be more reliable than normal facial expressions for conveying hidden emotional information.</b></p>

              <p>The aim of our MiGA workshop & challenge is to build a united, supportive research community for micro-gesture analysis and related emotion understanding problems. It will facilitate discussions between different research labs in academia and industry, identify the main attributes that can vary between gesture-based emotional understanding, and discuss the progress that has been made in this field so far, while identifying the next immediate open problems the community should address. We provide two different datasets and related benchmarks and with to inspire a new way of utilizing body gestures for human emotion understanding and bring a new direction to the emotion AI community. </p>

              <div class="profess-cover row no-margin"><div class="col-md-6"></div></div>

              <div>
                <!-- ===== Schedule & Logistics ===== -->
                <h4 id="schedule">Schedule & Logistics</h4>
                <div class="alert alert-light" role="alert" style="border: 1px solid #eee;">
                  <p style="margin:0;"><b>Dear IJCAI-25 MiGA Workshop Participants,</b></p>
                  <p>As we approach the IJCAI conference, we would like to share some important information about the schedules, equipment, and logistics for the MiGA Workshop 2025.</p>
                  <p><b>Schedule.</b> The official MiGA workshop schedule is now available below. Please check your presentation time to ensure you are well-prepared.</p>
                  <p><b>Presentation.</b> Due to the compact schedule (half day), each accepted paper will have an <b>8-minute oral presentation</b>, followed by a <b>2-minute Q&amp;A</b> session.</p>
                  <p><b>Equipment &amp; Help.</b> The following standard facilities will be provided for your presentation, and a student helper will be on standby to assist with AV systems:</p>
                  <ul>
                    <li>Projector &amp; projection screens</li>
                    <li>A (Windows) laptop</li>
                    <li>HDMI cable</li>
                  </ul>
                  <p>We strongly recommend that presenters bring their own adapters (HDMI, Type-C, etc.) to avoid any disruptions to the presentation.</p>
                  <p>We look forward to seeing you soon at the MiGA Workshop!</p>
                </div>

                <h5 class="mt-4">IJCAI 2025 Workshop Schedule</h5>
                <ul style="margin-left: 1rem;">
                  <li><b>Session 1: Introduction</b>
                    <ul>
                      <li>9:00–9:15: Opening Remarks</li>
                      <li>9:15–9:40: Invited Talk: Artificial Emotional Intelligence with Multi-model Large Language Models (Prof. Zheng Lian, CAS)</li>
                      <li>9:40–9:50: Award ceremony</li>
                    </ul>
                  </li>
                  <li class="mt-2"><b>Session 2: MG recognition and classification</b>
                    <ul>
                      <li>9:50–10:00: Paper 1 Presentation: MM-Gesture: Towards Precise Micro-Gesture Recognition through Multimodal Fusion</li>
                      <li>10:00–10:10: Paper 2 Presentation: Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention</li>
                      <li>10:10–10:20: Paper 3 Presentation: Online Micro-Gesture Recognition in Long Videos via Spatiotemporal Feature Encoding and Query-Based Temporal Detection</li>
                      <li>10:20–10:30: Paper 4 Presentation: CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset</li>
                    </ul>
                  </li>
                  <li class="mt-2">10:30–11:00: <i>First Coffee Break</i></li>
                  <li class="mt-2"><b>Session 3: Human behaviour-based emotion analysis</b>
                    <ul>
                      <li>11:00–11:10: Paper 5 Presentation: Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition</li>
                      <li>11:10–11:20: Paper 6 Presentation: Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks</li>
                      <li>11:20–11:30: Paper 7 Presentation: Towards Fine-Grained Emotion Understanding via Skeleton-Based Micro-Gesture Recognition</li>
                      <li>11:30–11:40: Paper 8 Presentation: GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation</li>
                      <li>11:40–11:50: Paper 9 Presentation: Unreal Engine-based Data Augmentation to Improve Real-world Human Activity Recognition with Wearable Devices</li>
                    </ul>
                  </li>
                  <li class="mt-2">12:00–12:20: Free-form discussion</li>
                  <li>12:10–12:30: Wrap-up/Closing Remarks</li>
                  <li>12:30–14:00: Lunch Break</li>
                </ul>
              </div>

              <h4 style="margin-top:16px !important;">Workshop Topics</h4>
              <ul>
                The workshop supplementing the challenge covers a wider scope, i.e., <b>any paper that is related to human behaviour analysis and emotion understanding</b> can be submitted as a workshop paper (not limited to challenge winners!). The topic includes but is not limited to:
                <div style="margin-left: 20px; text-align: justify;">
                  <li style="list-style: disc;">Human behaviour and gesture analysis.</li>
                  <li style="list-style: disc;">Vision-based methodologies for emotion understanding, e.g., classification, detection, online recognition, generation, and transferring.</li>
                  <li style="list-style: disc;">Solutions for special challenges involved with the in-the-wild human behaviour analysis, e.g., severely imbalanced sample distribution, high heterogeneous samples of interclass, noisy irrelevant motions, noisy backgrounds, etc.</li>
                  <li style="list-style: disc;">Different modalities developed for emotion understanding, e.g., body gestures, attentive gazes, and desensitized voices.</li>
                  <li style="list-style: disc;">New data collected for the purpose of emotion understanding.</li>
                  <li style="list-style: disc;">Psychological study and neuroscience research about various body behaviors and their links to emotions.</li>
                  <li style="list-style: disc;">Applications of human behaviour analysis, e.g., for medical assessment in hospitals (ADHD, depression), for health surveillance at home or in other environments, for emotion assessment in various scenarios like for education, job interview, etc.</li>
                </div>
              </ul>

              <br>
              <h4>References</h4>
              <ul>
                <li style="list-style: disc;">Chen H., Shi H., Liu X., Li X., and Zhao G. SMG: A Micro-gesture Dataset Towards Spontaneous Body Gestures for Emotional Stress State Analysis. International Journal of Computer Vision (IJCV 2023), 2023: 1-21.</li>
                <li style="list-style: disc;">Liu, X., Shi H., Chen H., Yu Z., Li X., and Zhao G. "iMiGUE: An identity-free video dataset for micro-gesture understanding and emotion analysis." In Proceedings of CVPR 2021, pp. 10631-10642. 2021.</li>
                <li style="list-style: disc;">Chen H., Liu X., Li X., Shi H., & Zhao G. Analyze spontaneous gestures for emotional stress state recognition: A micro-gesture dataset and analysis with deep learning. FG 2019: 1-8.</li>
              </ul>

              <br>
              <h4>Related works</h4>
              <ul>
                <li style="list-style: disc;">Huang H., Wang Y., Linghu K., and Xia Z. Multi-modal Micro-gesture Classification via Multi-scale Heterogeneous Ensemble Network. MiGA-IJCAI workshop 2024.</li>
                <li style="list-style: disc;">Chen G., Wang F., Li K., Wu Z., Fan H., Yang Y., Wang M and Guo D. Prototype Learning for Micro-gesture Classification. MiGA-IJCAI workshop 2024.</li>
                <li style="list-style: disc;">Guo D., Li K., Hu B., Zhang Y., Wang M. Benchmarking Micro-action Recognition: Dataset, Methods, and Applications. TCSVT 2024.</li>
                <li style="list-style: disc;">Wang, Y., Dong Z., Li P., Liu Y. A Multimodal Micro-gesture Classification Model Based on CLIP. MiGA-IJCAI workshop 2024.</li>
              </ul>
            </div>
          </div>
        </div>

        <!-- ===================== WORKSHOP TAB ===================== -->
        <div class="tab-pane fade exp-cover" id="profile" role="tabpanel" aria-labelledby="profile-tab">
          <!-- 原内容保持不变（此处省略；与之前版本一致） -->
          <!-- 你可以保留你已有的 Workshop Details、Schedule、Topics 等整段内容 -->
        </div>

        <!-- ===================== CHALLENGE TAB ===================== -->
        <div class="tab-pane fade exp-cover" id="resume" role="tabpanel" aria-labelledby="resume-tab">
          <!-- 原内容保持不变（此处省略；与之前版本一致） -->
        </div>

        <!-- ===================== ORGANISERS TAB ===================== -->
        <div class="tab-pane fade gallcoo" id="gallery" role="tabpanel" aria-labelledby="gallery-tab">
          <!-- 原内容保持不变（此处省略；与之前版本一致） -->
        </div>

        <!-- ===================== CONTACT TAB ===================== -->
        <div class="tab-pane fade contact-tab" id="contact" role="tabpanel" aria-labelledby="contact-tab">
          <!-- 原内容保持不变（此处省略；与之前版本一致） -->
        </div>
      </div>
    </div>
  </div>

  <script src="assets/js/jquery-3.2.1.min.js"></script>
  <script src="assets/js/popper.min.js"></script>
  <script src="assets/js/bootstrap.min.js"></script>
  <script src="assets/js/script.js"></script>
</body>
</html>